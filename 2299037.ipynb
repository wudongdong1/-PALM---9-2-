{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 飞桨常规赛：PALM眼底彩照中黄斑中央凹定位 - 9月第2名方案\n",
    "\n",
    "# 赛题介绍\n",
    "PALM黄斑定位常规赛的重点是研究和发展与患者眼底照片黄斑结构定位相关的算法。该常规赛的目标是评估和比较在一个常见的视网膜眼底图像数据集上定位黄斑的自动算法。具体目的是预测黄斑中央凹在图像中的坐标值。\n",
    "图\n",
    "\n",
    "# 数据简介\n",
    "PALM病理性近视预测常规赛由中山大学中山眼科中心提供800张带黄斑中央凹坐标标注的眼底彩照供选手训练模型，另提供400张带标注数据供平台进行模型测试。\n",
    "\n",
    "# 数据说明\n",
    "本次常规赛提供的金标准由中山大学中山眼科中心的7名眼科医生手工进行标注，之后由另一位高级专家将它们融合为最终的标注结果。本比赛提供数据集对应的黄斑中央凹坐标信息存储在xlsx文件中，名为“Fovea_Location_train”，第一列对应眼底图像的文件名(包括扩展名“.jpg”)，第二列包含x坐标，第三列包含y坐标。\n",
    "图\n",
    "\n",
    "# 训练数据集\n",
    "文件名称：Train\n",
    "Train文件夹里有一个文件夹fundus_images和一个xlsx文件。\n",
    "\n",
    "fundus_images文件夹内包含800张眼底彩照，分辨率为1444×1444，或2124×2056。命名形如H0001.jpg、P0001.jpg、N0001.jpg和V0001.jpg。\n",
    "xlsx文件中包含800张眼底彩照对应的x、y坐标信息。\n",
    "测试数据集\n",
    "文件名称：PALM-Testing400-Images 文件夹里包含400张眼底彩照，命名形如T0001.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 赛题重点难点\n",
    "这个项目解决的是一个回归问题，label是中央凹的坐标，通过卷积神经网络提取特征来提取中央凹坐标。难点是如果通过数据预处理，模型选择，loss选择，优化器选取，学习率调整来提高特征提取的准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 整体思路\n",
    "\n",
    "# 把常规赛：PALM眼底彩照中黄斑中央凹定位数据集解压到work文件夹下\n",
    "\n",
    "# 1、数据预处理\n",
    "去掉训练集和验证集里面的label是（0,0）的样本，resize成（224,224）,label是（0,0）如果直接作为训练样本，因为（0,0）并不是真实的坐标，直接加入训练会降低训练结果的准确性。\n",
    "通过改变亮度，扩展，裁剪，随即翻转，随机插值等进行数据增强\n",
    "```\n",
    "def random_distort(img):\n",
    "    \n",
    "    # 随机改变亮度\n",
    "    def random_brightness(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Brightness(img).enhance(e)\n",
    "    # 随机改变对比度\n",
    "    def random_contrast(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Contrast(img).enhance(e)\n",
    "    # 随机改变颜色\n",
    "    def random_color(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Color(img).enhance(e)\n",
    "\n",
    "    ops = [random_brightness, random_contrast, random_color]\n",
    "    np.random.shuffle(ops)\n",
    "\n",
    "    img = Image.fromarray(img)\n",
    "    img = ops[0](img)\n",
    "    img = ops[1](img)\n",
    "    img = ops[2](img)\n",
    "    img = np.asarray(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def random_expand(img,\n",
    "                  label,\n",
    "                  max_ratio=3.,\n",
    "                  fill=None,\n",
    "                  keep_ratio=True,\n",
    "                  thresh=0.5):\n",
    "    if random.random() > thresh:\n",
    "        return img, label\n",
    "\n",
    "    if max_ratio < 1.0:\n",
    "        return img, label\n",
    "\n",
    "    h, w, c = img.shape\n",
    "    ratio_x = random.uniform(1, max_ratio)\n",
    "    if keep_ratio:\n",
    "        ratio_y = ratio_x\n",
    "    else:\n",
    "        ratio_y = random.uniform(1, max_ratio)\n",
    "    oh = int(h * ratio_y)\n",
    "    ow = int(w * ratio_x)\n",
    "    off_x = random.randint(0, ow - w)\n",
    "    off_y = random.randint(0, oh - h)\n",
    "\n",
    "    out_img = np.zeros((oh, ow, c))\n",
    "    if fill and len(fill) == c:\n",
    "        for i in range(c):\n",
    "            out_img[:, :, i] = fill[i] * 255.0\n",
    "\n",
    "    out_img[off_y:off_y + h, off_x:off_x + w, :] = img\n",
    "    label[0] = ((label[0] * w) + off_x) / float(ow)\n",
    "    label[1] = ((label[1] * h) + off_y) / float(oh)\n",
    "\n",
    "\n",
    "    return out_img.astype('uint8'), label\n",
    "\n",
    "\n",
    "def random_crop(img,label):\n",
    "    if random.random() > 0.5:\n",
    "        return img, label\n",
    "    img1=img.copy()\n",
    "    label1=label.copy()\n",
    "    h,w,c=img1.shape\n",
    "    ow=int(0.7*w)\n",
    "    oh=int(0.7*h)\n",
    "    offx=random.randint(0, w-ow)\n",
    "    offy=random.randint(0, h-oh)\n",
    "    img2=img1[offy:offy+oh,offx:offx+ow]\n",
    "\n",
    "    label1[0] = ((label1[0] * w)-offx) / ow\n",
    "    label1[1] = ((label1[1] * h)-offy) / oh\n",
    "\n",
    "    if label1[0]<0 or label1[0]>1 or label1[1]<0 or label1[1]>1:\n",
    "        return img, label\n",
    "\n",
    "\n",
    "    return img2,label1\n",
    "\n",
    "\n",
    "\n",
    "def random_interp(img, size):\n",
    "    interp_method = [\n",
    "        cv2.INTER_NEAREST,\n",
    "        cv2.INTER_LINEAR,\n",
    "        cv2.INTER_AREA,\n",
    "        cv2.INTER_CUBIC,\n",
    "        cv2.INTER_LANCZOS4,\n",
    "    ]\n",
    "    n=random.randint(0,4)\n",
    "   \n",
    "    img = cv2.resize(img,(size,size),interpolation = interp_method[2])\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# 随机翻转\n",
    "def random_flip(img, label, thresh=0.5):\n",
    "    \n",
    "\n",
    "    if random.random() > thresh:\n",
    "        img = img[:, ::-1, :]\n",
    "        if label!=[0,0]:\n",
    "            label[0] = 1.0 - label[0]\n",
    "    if random.random() > thresh:\n",
    "        img = img[::-1, :, :]\n",
    "        if label!=[0,0]:\n",
    "            label[1] = 1.0 - label[1]\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "\n",
    "\n",
    "def image_augment(img, label, size, means=None):\n",
    "    # 随机改变亮暗、对比度和颜色等\n",
    "    img = random_distort(img)\n",
    "    # 随机填充\n",
    "    #img, label= random_expand(img, label, fill=means)\n",
    "    # 随机裁剪\n",
    "    img, label = random_crop(img, label)\n",
    "    # 随机缩放\n",
    "    img = random_interp(img, size)\n",
    "    # 随机翻转\n",
    "    img, label = random_flip(img, label)\n",
    "   \n",
    "\n",
    "    return img, label\n",
    "```\n",
    "\n",
    "# 2、模型主体搭建\n",
    "采用resnet101主体网络\n",
    "```\n",
    "class resnet_model(paddle.nn.Layer):\n",
    "    def __init__(self,num):\n",
    "        super(resnet_model,self).__init__()\n",
    "        self.model=resnet101(pretrained=True)\n",
    "        self.fc=Linear(1000,num)\n",
    "       \n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.model(x)\n",
    "        out=self.fc(out)\n",
    "        out=F.sigmoid(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# 3、loss选择\n",
    "\n",
    "```\n",
    "labelx=label[:,0]*w\n",
    "labely=label[:,1]*h\n",
    "            \n",
    "outx=out[:,0]*w\n",
    "outy=out[:,1]*h\n",
    "           \n",
    "square_x=F.square_error_cost(labelx,outx)\n",
    "square_y=F.square_error_cost(labely,outy)\n",
    "           \n",
    "sqrt_xy=paddle.sqrt(square_x+square_y)\n",
    "             \n",
    "distance=paddle.mean(sqrt_xy)\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    "# 4、优化器选择\n",
    "```\n",
    "lr = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=0.0001,T_max=int(640/batch_num) * epoches,verbose=False)\n",
    "opt = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters(),weight_decay=paddle.regularizer.L2Decay(0.0001))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 主体网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/blackhole/dataframe/typedef/typehints.py:157: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dt.stype.int32: [int, 'int', np.int, np.int32],\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/blackhole/dataframe/typedef/typehints.py:159: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dt.stype.float32: [float, 'float', np.float],\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/blackhole/dataframe/typedef/typehints.py:163: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dt.stype.bool8: [bool, 'boolean', 'bool', np.bool],\n",
      "100%|██████████| 263160/263160 [00:03<00:00, 71812.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from paddle.vision.models import resnet152,resnet50,resnet101\n",
    "from paddle.nn import Linear\n",
    "import paddle.nn.functional as F\n",
    "from paddle.optimizer import Momentum\n",
    "from paddle.regularizer import L2Decay\n",
    "from paddle.nn import CrossEntropyLoss\n",
    "from paddle.metric import Accuracy, Auc\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from paddle.vision.transforms import RandomHorizontalFlip,RandomVerticalFlip\n",
    "from PIL import ImageEnhance,Image\n",
    "import random\n",
    "import matplotlib.image as imgplt\n",
    "import blackhole.dataframe as df\n",
    "import warnings\n",
    "import pdb\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# 随机改变亮暗、对比度和颜色等\n",
    "def random_distort(img):\n",
    "    \n",
    "    # 随机改变亮度\n",
    "    def random_brightness(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Brightness(img).enhance(e)\n",
    "    # 随机改变对比度\n",
    "    def random_contrast(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Contrast(img).enhance(e)\n",
    "    # 随机改变颜色\n",
    "    def random_color(img, lower=0.5, upper=1.5):\n",
    "        e = np.random.uniform(lower, upper)\n",
    "        return ImageEnhance.Color(img).enhance(e)\n",
    "\n",
    "    ops = [random_brightness, random_contrast, random_color]\n",
    "    np.random.shuffle(ops)\n",
    "\n",
    "    img = Image.fromarray(img)\n",
    "    img = ops[0](img)\n",
    "    img = ops[1](img)\n",
    "    img = ops[2](img)\n",
    "    img = np.asarray(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def random_expand(img,\n",
    "                  label,\n",
    "                  max_ratio=3.,\n",
    "                  fill=None,\n",
    "                  keep_ratio=True,\n",
    "                  thresh=0.5):\n",
    "    if random.random() > thresh:\n",
    "        return img, label\n",
    "\n",
    "    if max_ratio < 1.0:\n",
    "        return img, label\n",
    "\n",
    "    h, w, c = img.shape\n",
    "    ratio_x = random.uniform(1, max_ratio)\n",
    "    if keep_ratio:\n",
    "        ratio_y = ratio_x\n",
    "    else:\n",
    "        ratio_y = random.uniform(1, max_ratio)\n",
    "    oh = int(h * ratio_y)\n",
    "    ow = int(w * ratio_x)\n",
    "    off_x = random.randint(0, ow - w)\n",
    "    off_y = random.randint(0, oh - h)\n",
    "\n",
    "    out_img = np.zeros((oh, ow, c))\n",
    "    if fill and len(fill) == c:\n",
    "        for i in range(c):\n",
    "            out_img[:, :, i] = fill[i] * 255.0\n",
    "\n",
    "    out_img[off_y:off_y + h, off_x:off_x + w, :] = img\n",
    "    label[0] = ((label[0] * w) + off_x) / float(ow)\n",
    "    label[1] = ((label[1] * h) + off_y) / float(oh)\n",
    "\n",
    "\n",
    "    return out_img.astype('uint8'), label\n",
    "\n",
    "\n",
    "def random_crop(img,label):\n",
    "    if random.random() > 0.5:\n",
    "        return img, label\n",
    "    img1=img.copy()\n",
    "    label1=label.copy()\n",
    "    h,w,c=img1.shape\n",
    "    ow=int(0.7*w)\n",
    "    oh=int(0.7*h)\n",
    "    offx=random.randint(0, w-ow)\n",
    "    offy=random.randint(0, h-oh)\n",
    "    img2=img1[offy:offy+oh,offx:offx+ow]\n",
    "\n",
    "    label1[0] = ((label1[0] * w)-offx) / ow\n",
    "    label1[1] = ((label1[1] * h)-offy) / oh\n",
    "\n",
    "    if label1[0]<0 or label1[0]>1 or label1[1]<0 or label1[1]>1:\n",
    "        return img, label\n",
    "\n",
    "\n",
    "    return img2,label1\n",
    "\n",
    "\n",
    "\n",
    "def random_interp(img, size):\n",
    "    interp_method = [\n",
    "        cv2.INTER_NEAREST,\n",
    "        cv2.INTER_LINEAR,\n",
    "        cv2.INTER_AREA,\n",
    "        cv2.INTER_CUBIC,\n",
    "        cv2.INTER_LANCZOS4,\n",
    "    ]\n",
    "    n=random.randint(0,4)\n",
    "   \n",
    "    img = cv2.resize(img,(size,size),interpolation = interp_method[2])\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# 随机翻转\n",
    "def random_flip(img, label, thresh=0.5):\n",
    "    \n",
    "\n",
    "    if random.random() > thresh:\n",
    "        img = img[:, ::-1, :]\n",
    "        if label!=[0,0]:\n",
    "            label[0] = 1.0 - label[0]\n",
    "    if random.random() > thresh:\n",
    "        img = img[::-1, :, :]\n",
    "        if label!=[0,0]:\n",
    "            label[1] = 1.0 - label[1]\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "\n",
    "\n",
    "def image_augment(img, label, size, means=None):\n",
    "    # 随机改变亮暗、对比度和颜色等\n",
    "    img = random_distort(img)\n",
    "    # 随机填充\n",
    "    #img, label= random_expand(img, label, fill=means)\n",
    "    # 随机裁剪\n",
    "    img, label = random_crop(img, label)\n",
    "    # 随机缩放\n",
    "    img = random_interp(img, size)\n",
    "    # 随机翻转\n",
    "    img, label = random_flip(img, label)\n",
    "   \n",
    "\n",
    "    return img, label\n",
    "\n",
    "\n",
    "data_dir = 'work/常规赛：PALM眼底彩照中黄斑中央凹定位/Train'\n",
    "data_dir1='work/常规赛：PALM眼底彩照中黄斑中央凹定位'\n",
    "\n",
    "def get_annotations():\n",
    "    train_dir=os.path.join(data_dir,'train.xlsx')\n",
    "    valid_dir=os.path.join(data_dir,'valid.xlsx')\n",
    "    test_dir=os.path.join(data_dir1,'PALM-Testing400-Images')\n",
    "    train_data = df.read_excel(train_dir)\n",
    "    valid_data=df.read_excel(valid_dir)\n",
    "    test_list=[]\n",
    "  \n",
    "    \n",
    "    train_values=train_data.values\n",
    "    valid_values=valid_data.values\n",
    "   \n",
    "    test=os.listdir(test_dir)\n",
    "    test_list=[[a,-1,-1] for a in test]\n",
    "    test_values=np.array(test_list,dtype=object)\n",
    "\n",
    "\n",
    "    return train_values,valid_values,test_values\n",
    "\n",
    "\n",
    "def get_image_data(value,mode='train'):\n",
    "    imageName=value[0]\n",
    "    label=value[1:3]\n",
    "    if mode == 'train' or mode=='valid':\n",
    "        imageDir=os.path.join(data_dir,'fundus_image',imageName)\n",
    "    if mode =='test':\n",
    "        imageDir=os.path.join(data_dir1,'PALM-Testing400-Images',imageName)\n",
    "    imageData=imgplt.imread(imageDir)\n",
    "    h,w,c=imageData.shape\n",
    "    label_normal=[label[0]/w,label[1]/h]\n",
    "    return imageData,label_normal,w,h\n",
    "\n",
    "\n",
    "def normalize(img,label):\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    #mean=[0.5,0.5,0.5]\n",
    "    #std=[0.5,0.5,0.5]\n",
    "\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    mean=np.array(mean).reshape((1,1,-1))\n",
    "    std=np.array(std).reshape((1,1,-1))\n",
    "    label=np.array(label).astype('float32')\n",
    "    img_Normal=(img/255.0-mean)/std\n",
    "    img_Normal=np.transpose(img_Normal,(2,0,1)).astype('float32')\n",
    "\n",
    "    return img_Normal,label\n",
    "\n",
    "\n",
    "class dataset(paddle.io.Dataset):\n",
    "    def __init__(self,mode='train',annotations=None):\n",
    "        super(dataset,self).__init__()\n",
    "        self.values=annotations\n",
    "        self.mode=mode\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        value=self.values[idx]\n",
    "        img,Label,w,h=get_image_data(value,self.mode)\n",
    "        if self.mode=='train':\n",
    "            img,Label=image_augment(img, Label, 224, means=None)\n",
    "        else:\n",
    "            img = cv2.resize(img,(224,224), interpolation =cv2.INTER_AREA)\n",
    "        image_normal,Label_normal=normalize(img,Label)\n",
    "        return image_normal,Label_normal,w,h\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "\n",
    "class resnet_model(paddle.nn.Layer):\n",
    "    def __init__(self,num):\n",
    "        super(resnet_model,self).__init__()\n",
    "        self.model=resnet101(pretrained=True)\n",
    "        self.fc=Linear(1000,num)\n",
    "       \n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.model(x)\n",
    "        out=self.fc(out)\n",
    "        out=F.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    batch_num=40\n",
    "    epoches=200\n",
    "    dataAnno=get_annotations()\n",
    "    lr = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=0.0001,\n",
    "                                                T_max=int(640/batch_num) * epoches,verbose=False)\n",
    "    opt = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters(),weight_decay=paddle.regularizer.L2Decay(0.0001))\n",
    "    \n",
    "    train_dataset=dataset(mode='train',annotations=dataAnno[0])\n",
    "    \n",
    "    train_loader=paddle.io.DataLoader(train_dataset, batch_size=batch_num, shuffle=True, num_workers=0, drop_last=False)\n",
    "    valid_dataset=dataset(mode='valid',annotations=dataAnno[1])\n",
    "    valid_loader=paddle.io.DataLoader(valid_dataset, batch_size=batch_num, shuffle=False, num_workers=0, drop_last=False)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    distances=[]\n",
    "    for epoch in range(epoches):\n",
    "\n",
    "        for batch_id,batch in enumerate(train_loader()):\n",
    "            x=batch[0]\n",
    "            w=batch[2]\n",
    "            h=batch[3]\n",
    "            label=batch[1]\n",
    "            out=model(x)\n",
    "            \n",
    "            \n",
    "            labelx=label[:,0]*w\n",
    "            labely=label[:,1]*h\n",
    "            \n",
    "            outx=out[:,0]*w\n",
    "            outy=out[:,1]*h\n",
    "           \n",
    "            square_x=F.square_error_cost(labelx,outx)\n",
    "            square_y=F.square_error_cost(labely,outy)\n",
    "           \n",
    "            sqrt_xy=paddle.sqrt(square_x+square_y)\n",
    "            \n",
    "            distance=paddle.mean(sqrt_xy)\n",
    "\n",
    "            loss=F.smooth_l1_loss(out,label)\n",
    "            avg_loss=paddle.mean(loss)\n",
    "\n",
    "\n",
    "            if batch_id % 2 == 0:\n",
    "                print(\"train:epoch: {}, batch_id: {}, loss is: {:.5f}\".format(epoch, batch_id, avg_loss.numpy()[0]))\n",
    "            # 反向传播，更新权重，清除梯度\n",
    "            distance.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "            lr.step()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "      \n",
    "       \n",
    "        losses1=[]\n",
    "        distance1=[]\n",
    "        for batch_id, data in enumerate(valid_loader()):\n",
    "            img=data[0]\n",
    "            w=data[2]\n",
    "            h=data[3]\n",
    "            label=data[1]\n",
    "            \n",
    "            out = model(img)\n",
    "           \n",
    "            labelx=label[:,0]*w\n",
    "            labely=label[:,1]*h\n",
    "            \n",
    "            outx=out[:,0]*w\n",
    "            outy=out[:,1]*h\n",
    "           \n",
    "            square_x=F.square_error_cost(labelx,outx)\n",
    "            square_y=F.square_error_cost(labely,outy)\n",
    "           \n",
    "            sqrt_xy=paddle.sqrt(square_x+square_y)\n",
    "            \n",
    "            distance=paddle.mean(sqrt_xy)\n",
    "           \n",
    "\n",
    "\n",
    "            \n",
    "            loss=F.smooth_l1_loss(out,label)\n",
    "            avg_loss=paddle.mean(loss)\n",
    "           \n",
    "            \n",
    "            #acc = paddle.metric.accuracy(pred, label1)\n",
    "            losses1.append(avg_loss.numpy()[0])\n",
    "            distance1.append(distance.numpy()[0])\n",
    "            #accuracies.append(acc.numpy())\n",
    "            \n",
    "            print(\"[validation] loss: {:.5f} ,Diatance is {:.5f}\".format(avg_loss.numpy()[0],distance.numpy()[0]))\n",
    "        losses.append(sum(losses1)/len(losses1))\n",
    "        distances.append(sum(distance1)/len(distance1))\n",
    "        epo=np.argmin(np.array(losses))\n",
    "        epo1=np.argmin(np.array(distances))\n",
    "        if epo1==epoch:\n",
    "            paddle.save(model.state_dict(), 'work/output/best_epoch'+str(epo1)+'.pdparams')\n",
    "            paddle.save(opt.state_dict(), 'work/output/best_epoch'+str(epo1)+'.pdopt')\n",
    "\n",
    "        print('验证集最小loss对应的epoch是：',epo,' loss值是：',losses[epo],' ; ','最小欧式距离对应的epoch是：',epo1,'distance是:',distances[epo1])\n",
    "        lossss=open('work/loss.txt','a+')\n",
    "        lossss.writelines('验证集最小loss对应的epoch是：'+str(epo)+' loss值是：'+str(losses[epo])+'  最小欧式距离对应的epoch是：'+str(epo1)+'  distance是:'+str(distances[epo1])+'\\n')\n",
    "        lossss.close()\n",
    "        model.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(model):\n",
    "   \n",
    "    dataAnno=get_annotations()\n",
    "    model_pararams=paddle.load('work/checkpoint/179.pdparams')\n",
    "    model.load_dict(model_pararams)\n",
    "    model.eval()\n",
    "    test_dataset=dataset(mode='test',annotations=dataAnno[2])\n",
    "    batch=10\n",
    "    test_loader=paddle.io.DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "    Fovea_XY=[]\n",
    "    sizeList=[]\n",
    "    for batch_id,batch in enumerate(test_loader()):\n",
    "        x=batch[0]      \n",
    "        sizeList.append(np.array([batch[2].numpy(),batch[3].numpy()]).transpose())\n",
    "        out=model(x)\n",
    "        result=out.numpy()\n",
    "        Fovea_XY.append(result)\n",
    "        print('预测到：batch',batch_id)\n",
    "\n",
    "    Fovea_XY=np.concatenate(Fovea_XY,axis=0)\n",
    "    sizeList=np.concatenate(sizeList,axis=0)\n",
    "    FileName=get_annotations()[2][:,0]\n",
    "    Fovea_XY=Fovea_XY*sizeList\n",
    "    Fovea_X=Fovea_XY[:,0]\n",
    "    Fovea_Y=Fovea_XY[:,1]\n",
    "   \n",
    "    submission = df.DataFrame(data={\n",
    "                            \"FileName\": FileName,\n",
    "                            \"Fovea_X\": Fovea_X,\n",
    "                            \"Fovea_Y\": Fovea_Y\n",
    "                        })\n",
    "    submission=submission.sort_values(by='FileName')\n",
    "    submission.to_csv(\"work/Fovea_Localization_Results.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "paddle.device.set_device('GPU:0')\n",
    "\n",
    "model=resnet_model(2)\n",
    "#train(model)\n",
    "predict(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# 飞桨使用体验及给其他选手的一些建议\n",
    "飞桨是一个非常好的深度学习平台，给大家提供免费的算力，免费的课程，各种常规比赛。里面的各种深度学习算法都有对应的模型，非常完备，工具也很齐全 给其他选手的一些建议： \n",
    "1.多参加平台上面的课程\n",
    "\n",
    "2.多参加比赛，实践中学习\n",
    "\n",
    "3.做项目的时候一定要多尝试各种模型，深入学习了解模型架构和原理。多尝试各种优化器，耐心调参，如果能理解各种优化器的原理和优缺点，那就更好了。\n",
    "\n",
    "4.要学会各种数据预处理的方法，只有有了好的数据，才能训练出一个好的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 参考资料\n",
    "[https://www.sciencedirect.com/science/article/abs/pii/S0169260717304145](http://)\n",
    "[https://agaldran.github.io/pdf/od_fovea_location.pdf](http://)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
